variant: M0
d_model: 512
n_heads: 8
e_layers: 2
d_ff: 2048
dropout: 0.1
activation: gelu
use_norm: true

# Optional features are disabled for the first M0 experiments.
meta:
  enabled: false
  mode: none
  source: none
  proj: linear
  mlp_hidden: 512

patch:
  enabled: false
  patch_len: 0
  mode: none
  local_win: 0
  use_pos_emb: false

multislot:
  # Internal embedding dimension for slotizer/temporal conv path.
  # If different from model.d_model, M0 will project slots to model.d_model before encoder.
  d_model: ${model.d_model}
  # Slot concat order is fixed by list order:
  # [p=8 slots 2] -> [p=32 slots 1] -> [p=L slots 1]
  scales:
    - 8
    - 32
    - ${data.seq_len}
  k_per_scale: [2, 1, 1]
  pad_value: 0.0
  temporal_conv:
    type: standard   # standard | dwsep
    kernel_size: 3
    layers: 2
    dilation: 1
    padding_mode: causal   # causal | same
  pma:
    n_heads: 8
    ffn: true
    kv_include_seeds: false
  slotizer:
    mode: pma   # pma | mean
    share_across_scales: false
    post_slot_attn:
      enabled: false
      n_heads: ${model.n_heads}
      share_across_scales: true  # if true, one shared layer; k=1 scales are skipped automatically
  diversity:
    enabled: false
    lambda: 0.0
    apply_to: x_only   # all | x_only
    min_patches: 2
    patch_renorm: true
    eps: 1e-8
  covariates:
    mode: slotize   # slotize | mlp_global
    mlp_hidden: ${model.d_model}
  cond_ln:
    enabled: false
    id: slot   # slot | scale
    apply_to: all   # all | x_only
  slot_attn:
    enabled: false
    mode: post   # post | interleave | between
    n_heads: ${model.n_heads}
  slot_fuse:
    mode: mlp   # mlp | residual_gated | residual_gated_output | scale_flatten_mean | select_global
    hidden: ${model.d_ff}
    score_mode: full   # full | lowrank (for residual_gated / residual_gated_output scorer)
    score_rank: 0      # used when score_mode=lowrank
    alpha_source: repr   # repr | output_attn (for residual_gated_output, when horizon_alpha is disabled)
    output_attn_rank: 64
    output_attn_use_delta_key: true
    output_attn_norm: layernorm   # layernorm | l2 | none
    output_attn_tau: 1.0
    extra_slot_dropout: 0.0   # train-time dropout on extra slots only (baseline slot is never dropped)
    horizon_gate:
      enabled: false
      mode: linear   # linear | bias
      init_schedule: linear   # linear | sigmix_v1
      init_start: -2.0
      init_end: -4.0
    horizon_alpha:
      enabled: false
      hidden: 16
    global_ma:
      enabled: false
      kernel_size: 25
      combine: gated_add   # add | gated_add
